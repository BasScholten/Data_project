{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import hamming\n",
    "from scipy.fft import fft, rfft, ifft\n",
    "from scipy.stats import *\n",
    "from scipy.signal import butter, freqs, sosfilt\n",
    "import sqlalchemy\n",
    "import matplotlib.pyplot as plt\n",
    "from seaborn import kdeplot, distplot, displot, histplot\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3686810724.py, line 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [2]\u001b[1;36m\u001b[0m\n\u001b[1;33m    return i\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## Import data \n",
    "import glob                \n",
    "import os\n",
    "##Start with low_freq\n",
    "#Exists of house_1 to house_6\n",
    "#Each house_x map cosist of :\n",
    "#channel_x.dat (where x 1 till x)\n",
    "#And a file labels.dat\n",
    "\n",
    "\n",
    "def house_import(houseid):\n",
    "    path=\"r\\Data\\low_freq\\house_{0}\".format(houseid)\n",
    "    list = os.listdir(path)      # dir is your directory path\n",
    "    number_files = len(list)\n",
    "\n",
    "    for i in range(1,number_files-1):\n",
    "        i= pd.read_csv(path.join(\"\\channel_{0}.dat\".format(i))\n",
    "\n",
    "    return        \n",
    "\n",
    "house_import(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## I'm an idiot and cannot work with H5, so I abbondoned this route.\n",
    "#from nilmtk.dataset_converters import convert_redd\n",
    "#from nilmtk import DataSet\n",
    "#import nilmtk\n",
    "#https://github.com/nilmtk/nilmtk/tree/master/docs/manual used\n",
    "\n",
    "##Convert data\n",
    "#First decompress the data I used 7zip (https://www.7-zip.org/)\n",
    "#convert_redd('Data\\low_freq', 'Data\\low_freq.h5')\n",
    "#convert_redd('Data\\high_freq', 'Data\\high_freq.h5')            Gives error, no clue why, but I decided to focus on the low freq first anyway (more usefull anyway)\n",
    "#convert_redd('Data\\high_freq_raw', 'Data\\high_freq_raw.h5')    Gives error, no clue why, but I decided to focus on the low freq first anyway (more usefull anyway)\n",
    "\n",
    "##Import the data\n",
    "#low_freq = DataSet('Data\\low_freq.h5')\n",
    "\n",
    "##What's in it? \n",
    "#low_freq.metadata\n",
    "#https://www.codetd.com/en/article/13005539 as inspiration\n",
    "\n",
    "#elec=low_freq.elecs           #Building 1 (of 6) and electric\n",
    "#elec.__globals__\n",
    "\n",
    "\n",
    "#top_5_train_elec = train_elec.submeters().select_top_k(k=5)\n",
    "\n",
    "#def Get_chunks(train, sample_period):   ## Function predicting\n",
    "    #Initialization\n",
    "#    gt= {}\n",
    "    \n",
    "    #Loading data\n",
    "#    for i, chunk in enumerate(train.mains().load(sample_period=sample_period)):\n",
    "#        chunk_drop_na = chunk.dropna()   ### No NaN values\n",
    "#        gt[i]={}  ## Actual power\n",
    "\n",
    "#        for meter in train.submeters().meters:\n",
    "            # Only use the meters that we trained on (this saves time!)    \n",
    "#            gt[i][meter] = next(meter.load(sample_period=sample_period))  \n",
    "#        gt[i] = pd.DataFrame({k:v.squeeze() for k,v in gt[i].items()}, index=next(iter(gt[i].values())).index).dropna()   #### Convert to dataframe\n",
    "\n",
    "    # If everything can fit in memory\n",
    "#    gt_overall = pd.concat(gt)   \n",
    "#    gt_overall.index = gt_overall.index.droplevel()\n",
    "#    return gt_overall\n",
    "\n",
    "#predictions = {}\n",
    "#classifiers = {'FHMM':FHMM()}\n",
    "#sample_period = 120  ## Sampling period in s\n",
    "#df = Get_chunks(train, 120)\n",
    "\n",
    "#def compute_rmse(gt, pred):   ### calculate rmse\n",
    "#    from sklearn.metrics import mean_squared_error\n",
    "#    rms_error = {}\n",
    "#    for appliance in gt.columns:\n",
    "#        rms_error[appliance] = np.sqrt(mean_squared_error(gt[appliance], pred[appliance])) ## rootmean square error\n",
    "#    return pd.Series(rms_error)\n",
    "#rmse = {}\n",
    "\n",
    "#for clf_name in classifiers.keys():\n",
    " #   rmse[clf_name] = compute_rmse(gt, predictions[clf_name])\n",
    "\n",
    "#rmse = pd.DataFrame(rmse)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python (nilmtk-env)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
