{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import peakutils\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier(df, margin):\n",
    "    std = df.std()\n",
    "    margin_abs = std * margin\n",
    "    prev = df.shift(1)\n",
    "    next = df.shift(-1)\n",
    "    df[((df-prev) >  margin_abs) & ((df-next) > margin_abs)] = np.nan\n",
    "    df[((prev-df) >  margin_abs) & ((next-df) > margin_abs)] = np.nan\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_and_drop_day(data, gap_size):\n",
    "    \n",
    "    # add a new column with the time differences between consecutive rows\n",
    "    data['time_diff'] = data.index.to_series().diff().dt.total_seconds().fillna(0)\n",
    "    \n",
    "    # get the dates to drop\n",
    "    dates_to_drop = data[data['time_diff'] > gap_size].index.date\n",
    "\n",
    "    # create DatetimeIndex object from the array of dates\n",
    "    dates_to_drop = pd.DatetimeIndex(dates_to_drop)\n",
    "\n",
    "    # drop the rows with the dates to drop\n",
    "    data = data.loc[~data.index.isin(dates_to_drop)]\n",
    "\n",
    "    # drop the time_diff column\n",
    "    data = data.drop('time_diff', axis=1, level=0)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_house_data(appliance_data, gap_size, house_ids):\n",
    "    # List comprehension to create separate dataframes for each house\n",
    "    house_dfs = [onehouse(appliance_data, i) for i in house_ids]\n",
    "\n",
    "    # Apply the dropdate() function to each dataframe\n",
    "    for i, house_df in enumerate(house_dfs):\n",
    "        house_df.reset_index()\n",
    "        print(f\"House {i+1} before cleaning:\")\n",
    "        print(house_df.isna().sum())\n",
    "        house_df = check_and_drop_day(house_df, gap_size)\n",
    "        house_df = outlier(house_df, 1.5)\n",
    "        house_dfs[i] = house_df\n",
    "        print(f\"House {i+1} after cleaning:\")\n",
    "        print(house_df.isna().sum())\n",
    "\n",
    "    # Name the dataframes\n",
    "    for i, house_df in enumerate(house_dfs):\n",
    "        globals()[f\"house_{i+1}\"] = house_df.copy()\n",
    "    \n",
    "    return house_dfs\n",
    "\n",
    "# Define the gap size threshold in seconds\n",
    "gap_size = 60\n",
    "\n",
    "# List of house IDs\n",
    "house_ids = [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "# Call the function to clean the data\n",
    "cleaned_house_dfs = clean_house_data(appliance_data, gap_size, house_ids)\n",
    "\n",
    "# Print the number of NaN values for each house before and after cleaning\n",
    "for i, house_df in enumerate(cleaned_house_dfs):\n",
    "    print(f\"House {i+1} before cleaning:\")\n",
    "    print(house_df.isna().sum().sum())\n",
    "    print(f\"House {i+1} after cleaning:\")\n",
    "    print(house_df.isna().sum().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# calculate FFT of mains signal for each 5-minute interval\n",
    "fft_data = []\n",
    "for i in range(len(mains_data)//(5*60)):\n",
    "    fft = np.fft.fft(mains_data[i*5*60:(i+1)*5*60])\n",
    "    fft_data.append(np.abs(fft[:len(fft)//2]))\n",
    "\n",
    "# create 2D array of FFT magnitudes\n",
    "fft_data = np.array(fft_data)\n",
    "num_freq_bins = fft_data.shape[1]\n",
    "\n",
    "# reshape to 3D array\n",
    "fft_data = fft_data.reshape((1, fft_data.shape[0], num_freq_bins))\n",
    "\n",
    "# define CNN model\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Conv1D(32, kernel_size=3, activation='relu', input_shape=(fft_data.shape[1], fft_data.shape[2])),\n",
    "    layers.MaxPooling1D(pool_size=2),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# compile and train model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(fft_data, labels, epochs=10, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def prepare_lstm_input(float_series, label_series, sequence_length, train_ratio=0.8):\n",
    "    # Combine float and label series\n",
    "    combined_data = np.hstack((float_series, label_series))\n",
    "\n",
    "    # Normalize the float series\n",
    "    scaler = MinMaxScaler()\n",
    "    combined_data[:, :float_series.shape[1]] = scaler.fit_transform(float_series)\n",
    "\n",
    "    # Split the combined data into input sequences and target labels\n",
    "    num_samples = len(combined_data) - sequence_length + 1\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(num_samples):\n",
    "        sequences.append(combined_data[i:i+sequence_length, :float_series.shape[1]])\n",
    "        targets.append(combined_data[i+sequence_length-1, -1])\n",
    "\n",
    "    # Convert sequences and targets to numpy arrays\n",
    "    sequences = np.array(sequences)\n",
    "    targets = np.array(targets).reshape(-1, 1)\n",
    "\n",
    "    # Split the data into train and test sets\n",
    "    split_index = int(train_ratio * num_samples)\n",
    "    x_train, y_train = sequences[:split_index], targets[:split_index]\n",
    "    x_test, y_test = sequences[split_index:], targets[split_index:]\n",
    "\n",
    "    return x_train, y_train, x_test, y_test, scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the LSTM model\n",
    "seq_length = 15\n",
    "i = 1               #target column, 1 in curent form\n",
    "train_size = 0.75\n",
    "val_size = 0.75\n",
    "\n",
    "\n",
    "\n",
    "x_train, y_train, x_val, y_val, x_test, y_test, test_dt, train_dt, val_dt, scaler = split_data(pd.merge(y,x, left_index=True, right_index=True), i, seq_length, train_size, val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM model architecture\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(seq_length, x_train.shape[2]), activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1, activation='sigmoid'))  # For binary classification, use sigmoid activation\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')  # Use binary cross-entropy for binary classification\n",
    "\n",
    "# Set early stopping callback to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, mode='min')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(x_train, y_train, epochs=100, batch_size=64, validation_data=(x_val, y_val), callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on the test set\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "# reshape y_test to have shape (16653,)\n",
    "y_test = y_test[:, 0]\n",
    "\n",
    "# Reshape y_test and y_pred to 2D tensors\n",
    "y_test_2d = np.reshape(y_test, (-1, 1))\n",
    "y_pred_2d = np.reshape(y_pred, (-1, 1))\n",
    "\n",
    "\n",
    "#inverse transform\n",
    "y_test_trans = scaler.inverse_transform(y_test_2d)\n",
    "y_pred_trans = scaler.inverse_transform(y_pred_2d)\n",
    "\n",
    "# Calculate mean squared error\n",
    "mse = np.mean(np.square(y_pred - y_test))\n",
    "\n",
    "print(\"Mean squared error:\", mse)\n",
    "\n",
    "\n",
    "# Plot predicted values against true values\n",
    "plt.scatter(y_test_2d, y_pred_2d)\n",
    "plt.xlabel(\"True Values\")\n",
    "plt.ylabel(\"Predictions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(history):\n",
    "   plt.figure(figsize=(8,4))\n",
    "   plt.plot(history.history['loss'], label='Train Loss')\n",
    "   plt.plot(history.history['val_loss'], label='Test Loss')\n",
    "   plt.title('model loss')\n",
    "   plt.ylabel('loss')\n",
    "   plt.xlabel('epochs')\n",
    "   plt.legend(loc='upper right')\n",
    "   plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = model.evaluate(x_train, y_train, verbose=0)\n",
    "print('Train Root Mean Squared Error(RMSE): %.2f; Train Mean Absolute Error(MAE) : %.2f ' \n",
    "    % (np.sqrt(train_score), train_score))\n",
    "\n",
    "test_score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test Root Mean Squared Error(RMSE): %.2f; Test Mean Absolute Error(MAE) : %.2f ' \n",
    "    % (np.sqrt(test_score), test_score))\n",
    "model_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_plot(y_test, test_predict, dt):\n",
    "   # Convert index to datetime objects\n",
    "   dt = pd.to_datetime(dt)\n",
    "   fig, ax = plt.subplots()\n",
    "   ax.plot(dt, y_test[:], marker='.', label=\"actual\")\n",
    "   ax.plot(dt, test_predict[:], 'r', label=\"prediction\")\n",
    "   ax.set_ylabel('Ads Daily Spend', size=15)\n",
    "   ax.set_xlabel('Date', size=15)\n",
    "\n",
    "   # Choose 5 ticks on the x-axis\n",
    "   n_ticks = 5\n",
    "   tick_locs = np.linspace(0, len(dt)-1, n_ticks, dtype=int)\n",
    "   ax.set_xticks(dt[tick_locs])\n",
    "   ax.set_xticklabels(dt[tick_locs].strftime('%Y-%m-%d'), rotation=45)\n",
    "   ax.legend(fontsize=15)\n",
    "   plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_plot(y_test,y_pred, test_dt)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2be5faf79681da6f2a61fdfdd5405d65d042280f7fba6178067603e3a2925119"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
